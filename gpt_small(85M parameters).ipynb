{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOSvsw5z2rm61Ke3rL/prjT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeep-swain/gpt2-small/blob/main/gpt_small(85M%20parameters).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "qJeToX32Y5oY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "URTf4aLdF0wd"
      },
      "outputs": [],
      "source": [
        "with open('/content/eng-poem.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Zo83kfKCF-Kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ek5vqwGIIk5",
        "outputId": "dd8bed58-c6e9-4094-b7a3-a6ba148e52b9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  3145728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fkoxg6nOILKj",
        "outputId": "ba08d062-efd8-44f0-a354-a72408801afc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \"%'()+,-/0123456789:=@[\\]_`abcdefghijklmnopqrstuvwxyz{}~\n",
            "57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "def decode(l):\n",
        "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ],
      "metadata": {
        "id": "m0dA2lIiIUCQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = len(text)\n",
        "train_data = text[:int(n*0.9)]\n",
        "val_data = text[int(n*0.9):]"
      ],
      "metadata": {
        "id": "aRqiwAUh2c-K"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids = encode(train_data)\n",
        "val_ids = encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile('train.bin')\n",
        "val_ids.tofile('val.bin')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXRtlT-G2eM7",
        "outputId": "09cad6ed-8d65-443d-be78-64a1a572d9c9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 2,831,155 tokens\n",
            "val has 314,573 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''model'''\n",
        "\n",
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        # with weight tying when using torch.compile() some warnings get generated:\n",
        "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
        "        # This behavior is deprecated and will be an error in future versions\"\n",
        "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        # model surgery to decrease the block size if necessary\n",
        "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
        "        # but want to use a smaller block size for some smaller, simpler model\n",
        "        assert block_size <= self.config.block_size\n",
        "        self.config.block_size = block_size\n",
        "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, 'bias'):\n",
        "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        override_args = override_args or {} # default to empty dict\n",
        "        # only dropout can be overridden see more notes below\n",
        "        assert all(k == 'dropout' for k in override_args)\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        config_args['bias'] = True # always True for GPT model checkpoints\n",
        "        # we can override the dropout rate, if desired\n",
        "        if 'dropout' in override_args:\n",
        "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
        "            config_args['dropout'] = override_args['dropout']\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ],
      "metadata": {
        "id": "74V9t7t13oCz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''train'''\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "\n",
        "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
        "# I/O\n",
        "out_dir = 'out'\n",
        "eval_interval = 2000\n",
        "log_interval = 1\n",
        "eval_iters = 200\n",
        "eval_only = False # if True, script exits right after the first eval\n",
        "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
        "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
        "# wandb logging\n",
        "wandb_log = False # disabled by default\n",
        "wandb_project = 'owt'\n",
        "wandb_run_name = 'gpt2' # 'run' + str(time.time())\n",
        "# data\n",
        "dataset = '/content/eng-poem.txt.json'\n",
        "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
        "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
        "block_size = 1024\n",
        "# model\n",
        "n_layer = 12\n",
        "n_head = 12\n",
        "n_embd = 768\n",
        "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
        "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
        "# adamw optimizer\n",
        "learning_rate = 6e-4 # max learning rate\n",
        "max_iters = 600000 # total number of training iterations\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
        "# learning rate decay settings\n",
        "decay_lr = True # whether to decay the learning rate\n",
        "warmup_iters = 2000 # how many steps to warm up for\n",
        "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
        "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
        "\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "#exec(open('configurator.py').read()) # overrides from command line or config file\n",
        "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
        "\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank # each process gets a different seed\n",
        "    # world_size number of processes will be training simultaneously, so we can scale\n",
        "    # down the desired gradient accumulation iterations per process proportionally\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "train_data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap( 'val.bin', dtype=np.uint16, mode='r')\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "# model init\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=vocab_size, dropout=dropout)\n",
        "\n",
        "if init_from == 'scratch':\n",
        "    # init a new model from scratch\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    # determine the vocab size we'll use for from-scratch training\n",
        "    # if meta_vocab_size is None:\n",
        "    #     print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "    #model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "elif init_from == 'resume':\n",
        "    print(f\"Resuming training from {out_dir}\")\n",
        "    # resume training from a checkpoint.\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    checkpoint_model_args = checkpoint['model_args']\n",
        "    # force these config attributes to be equal otherwise we can't even resume training\n",
        "    # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = checkpoint_model_args[k]\n",
        "    # create the model\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    # fix the keys of the state dictionary :(\n",
        "    # honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "    iter_num = checkpoint['iter_num']\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "elif init_from.startswith('gpt2'):\n",
        "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
        "    # initialize from OpenAI GPT-2 weights\n",
        "    override_args = dict(dropout=dropout)\n",
        "    model = GPT.from_pretrained(init_from, override_args)\n",
        "    # read off the created config params, so we can store them into checkpoint correctly\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = getattr(model.config, k)\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
        "model.to(device)\n",
        "\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# optimizer\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "if init_from == 'resume':\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "checkpoint = None # free up memory\n",
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "\n",
        "# training loop\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100, # convert to percentage\n",
        "            })\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C65NwTBXDvSG",
        "outputId": "89c714cf-e6c5-4632-e9f2-39003cf02934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 85.00M\n",
            "num decayed parameter tensors: 50, with 85,764,864 parameters\n",
            "num non-decayed parameter tensors: 25, with 19,200 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2023-12-17 09:25:22,694] torch._dynamo.convert_frame: [WARNING] WON'T CONVERT forward <ipython-input-11-9de728b842cf> line 163 \n",
            "[2023-12-17 09:25:22,694] torch._dynamo.convert_frame: [WARNING] due to: \n",
            "[2023-12-17 09:25:22,694] torch._dynamo.convert_frame: [WARNING] Traceback (most recent call last):\n",
            "[2023-12-17 09:25:22,694] torch._dynamo.convert_frame: [WARNING]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "[2023-12-17 09:25:22,694] torch._dynamo.convert_frame: [WARNING]     raise self._exception\n",
            "[2023-12-17 09:25:22,694] torch._dynamo.convert_frame: [WARNING] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "[2023-12-17 09:25:22,694] torch._dynamo.convert_frame: [WARNING] AssertionError: libcuda.so cannot found!\n",
            "[2023-12-17 09:25:22,694] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:22,694] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:22,694] torch._dynamo.convert_frame: [WARNING] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "[2023-12-17 09:25:22,694] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:22,694] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:23,819] torch._dynamo.convert_frame: [WARNING] WON'T CONVERT forward <ipython-input-11-9de728b842cf> line 96 \n",
            "[2023-12-17 09:25:23,819] torch._dynamo.convert_frame: [WARNING] due to: \n",
            "[2023-12-17 09:25:23,819] torch._dynamo.convert_frame: [WARNING] Traceback (most recent call last):\n",
            "[2023-12-17 09:25:23,819] torch._dynamo.convert_frame: [WARNING]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "[2023-12-17 09:25:23,819] torch._dynamo.convert_frame: [WARNING]     raise self._exception\n",
            "[2023-12-17 09:25:23,819] torch._dynamo.convert_frame: [WARNING] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "[2023-12-17 09:25:23,819] torch._dynamo.convert_frame: [WARNING] AssertionError: libcuda.so cannot found!\n",
            "[2023-12-17 09:25:23,819] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:23,819] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:23,819] torch._dynamo.convert_frame: [WARNING] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "[2023-12-17 09:25:23,819] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:23,819] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:23,939] torch._dynamo.convert_frame: [WARNING] WON'T CONVERT forward <ipython-input-11-9de728b842cf> line 19 \n",
            "[2023-12-17 09:25:23,939] torch._dynamo.convert_frame: [WARNING] due to: \n",
            "[2023-12-17 09:25:23,939] torch._dynamo.convert_frame: [WARNING] Traceback (most recent call last):\n",
            "[2023-12-17 09:25:23,939] torch._dynamo.convert_frame: [WARNING]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "[2023-12-17 09:25:23,939] torch._dynamo.convert_frame: [WARNING]     raise self._exception\n",
            "[2023-12-17 09:25:23,939] torch._dynamo.convert_frame: [WARNING] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "[2023-12-17 09:25:23,939] torch._dynamo.convert_frame: [WARNING] AssertionError: libcuda.so cannot found!\n",
            "[2023-12-17 09:25:23,939] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:23,939] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:23,939] torch._dynamo.convert_frame: [WARNING] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "[2023-12-17 09:25:23,939] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:23,939] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:24,258] torch._dynamo.convert_frame: [WARNING] WON'T CONVERT forward <ipython-input-11-9de728b842cf> line 45 \n",
            "[2023-12-17 09:25:24,258] torch._dynamo.convert_frame: [WARNING] due to: \n",
            "[2023-12-17 09:25:24,258] torch._dynamo.convert_frame: [WARNING] Traceback (most recent call last):\n",
            "[2023-12-17 09:25:24,258] torch._dynamo.convert_frame: [WARNING]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "[2023-12-17 09:25:24,258] torch._dynamo.convert_frame: [WARNING]     raise self._exception\n",
            "[2023-12-17 09:25:24,258] torch._dynamo.convert_frame: [WARNING] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "[2023-12-17 09:25:24,258] torch._dynamo.convert_frame: [WARNING] AssertionError: libcuda.so cannot found!\n",
            "[2023-12-17 09:25:24,258] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:24,258] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:24,258] torch._dynamo.convert_frame: [WARNING] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "[2023-12-17 09:25:24,258] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:24,258] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:24,543] torch._dynamo.convert_frame: [WARNING] WON'T CONVERT forward <ipython-input-11-9de728b842cf> line 80 \n",
            "[2023-12-17 09:25:24,543] torch._dynamo.convert_frame: [WARNING] due to: \n",
            "[2023-12-17 09:25:24,543] torch._dynamo.convert_frame: [WARNING] Traceback (most recent call last):\n",
            "[2023-12-17 09:25:24,543] torch._dynamo.convert_frame: [WARNING]   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "[2023-12-17 09:25:24,543] torch._dynamo.convert_frame: [WARNING]     raise self._exception\n",
            "[2023-12-17 09:25:24,543] torch._dynamo.convert_frame: [WARNING] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "[2023-12-17 09:25:24,543] torch._dynamo.convert_frame: [WARNING] AssertionError: libcuda.so cannot found!\n",
            "[2023-12-17 09:25:24,543] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:24,543] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:24,543] torch._dynamo.convert_frame: [WARNING] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "[2023-12-17 09:25:24,543] torch._dynamo.convert_frame: [WARNING] \n",
            "[2023-12-17 09:25:24,543] torch._dynamo.convert_frame: [WARNING] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1481, val loss 4.1453\n",
            "iter 0: loss 4.1415, time 99229.25ms, mfu -100.00%\n",
            "iter 1: loss 4.1505, time 19607.74ms, mfu -100.00%\n",
            "iter 2: loss 4.0892, time 20522.33ms, mfu -100.00%\n",
            "iter 3: loss 3.9941, time 20287.77ms, mfu -100.00%\n",
            "iter 4: loss 3.8893, time 20132.18ms, mfu -100.00%\n",
            "iter 5: loss 3.7283, time 20259.70ms, mfu 4.85%\n",
            "iter 6: loss 3.6070, time 20333.03ms, mfu 4.84%\n",
            "iter 7: loss 3.4732, time 20338.09ms, mfu 4.84%\n",
            "iter 8: loss 3.3891, time 20232.25ms, mfu 4.84%\n",
            "iter 9: loss 3.3330, time 20234.47ms, mfu 4.84%\n",
            "iter 10: loss 3.2569, time 20197.49ms, mfu 4.85%\n",
            "iter 11: loss 3.1810, time 20124.14ms, mfu 4.85%\n",
            "iter 12: loss 3.1374, time 20095.38ms, mfu 4.85%\n",
            "iter 13: loss 3.1334, time 20103.31ms, mfu 4.86%\n",
            "iter 14: loss 3.0746, time 20101.33ms, mfu 4.86%\n",
            "iter 15: loss 3.0378, time 20112.01ms, mfu 4.86%\n",
            "iter 16: loss 3.0147, time 20119.79ms, mfu 4.86%\n",
            "iter 17: loss 2.9679, time 20156.27ms, mfu 4.86%\n",
            "iter 18: loss 2.9625, time 20150.20ms, mfu 4.86%\n",
            "iter 19: loss 2.9102, time 20148.25ms, mfu 4.87%\n",
            "iter 20: loss 2.8494, time 20161.91ms, mfu 4.87%\n",
            "iter 21: loss 2.8638, time 20162.44ms, mfu 4.87%\n",
            "iter 22: loss 2.7629, time 20207.05ms, mfu 4.87%\n",
            "iter 23: loss 2.7216, time 20234.38ms, mfu 4.86%\n",
            "iter 24: loss 2.7210, time 20293.39ms, mfu 4.86%\n",
            "iter 25: loss 2.7301, time 20330.50ms, mfu 4.86%\n",
            "iter 26: loss 2.6617, time 20339.60ms, mfu 4.86%\n",
            "iter 27: loss 2.6367, time 20315.77ms, mfu 4.85%\n",
            "iter 28: loss 2.6326, time 20326.50ms, mfu 4.85%\n",
            "iter 29: loss 2.6190, time 20284.42ms, mfu 4.85%\n",
            "iter 30: loss 2.5883, time 20274.91ms, mfu 4.85%\n",
            "iter 31: loss 2.5732, time 20264.51ms, mfu 4.85%\n",
            "iter 32: loss 2.5707, time 20245.03ms, mfu 4.85%\n",
            "iter 33: loss 2.5466, time 20225.31ms, mfu 4.85%\n",
            "iter 34: loss 2.5478, time 20286.10ms, mfu 4.85%\n",
            "iter 35: loss 2.5236, time 20286.64ms, mfu 4.85%\n",
            "iter 36: loss 2.5161, time 20248.49ms, mfu 4.85%\n",
            "iter 37: loss 2.4990, time 20254.29ms, mfu 4.85%\n",
            "iter 38: loss 2.4895, time 20204.81ms, mfu 4.85%\n",
            "iter 39: loss 2.4970, time 20190.61ms, mfu 4.85%\n",
            "iter 40: loss 2.5468, time 20185.57ms, mfu 4.85%\n",
            "iter 41: loss 2.4997, time 20179.91ms, mfu 4.85%\n",
            "iter 42: loss 2.4966, time 20167.75ms, mfu 4.85%\n",
            "iter 43: loss 2.4828, time 20146.72ms, mfu 4.86%\n",
            "iter 44: loss 2.4755, time 20186.55ms, mfu 4.86%\n",
            "iter 45: loss 2.4773, time 20212.54ms, mfu 4.86%\n",
            "iter 46: loss 2.4592, time 20225.12ms, mfu 4.86%\n",
            "iter 47: loss 2.4463, time 20234.35ms, mfu 4.86%\n",
            "iter 48: loss 2.4391, time 20229.94ms, mfu 4.86%\n",
            "iter 49: loss 2.4441, time 20226.66ms, mfu 4.86%\n",
            "iter 50: loss 2.3860, time 20190.47ms, mfu 4.86%\n",
            "iter 51: loss 2.4331, time 20157.95ms, mfu 4.86%\n",
            "iter 52: loss 2.4121, time 20158.43ms, mfu 4.86%\n",
            "iter 53: loss 2.4378, time 20111.65ms, mfu 4.86%\n",
            "iter 54: loss 2.4252, time 20127.55ms, mfu 4.86%\n",
            "iter 55: loss 2.4016, time 20171.14ms, mfu 4.86%\n",
            "iter 56: loss 2.4411, time 20172.01ms, mfu 4.86%\n",
            "iter 57: loss 2.4462, time 20176.80ms, mfu 4.86%\n",
            "iter 58: loss 2.4170, time 20178.19ms, mfu 4.86%\n",
            "iter 59: loss 2.4080, time 20201.34ms, mfu 4.86%\n",
            "iter 60: loss 2.3842, time 20212.61ms, mfu 4.86%\n",
            "iter 61: loss 2.4029, time 20222.66ms, mfu 4.86%\n",
            "iter 62: loss 2.3990, time 20201.64ms, mfu 4.86%\n",
            "iter 63: loss 2.4286, time 20210.26ms, mfu 4.86%\n",
            "iter 64: loss 2.3869, time 20186.85ms, mfu 4.86%\n",
            "iter 65: loss 2.4051, time 20214.59ms, mfu 4.86%\n",
            "iter 66: loss 2.4191, time 20233.80ms, mfu 4.86%\n",
            "iter 67: loss 2.4199, time 20229.53ms, mfu 4.86%\n",
            "iter 68: loss 2.4041, time 20210.71ms, mfu 4.86%\n",
            "iter 69: loss 2.3954, time 20209.64ms, mfu 4.86%\n",
            "iter 70: loss 2.3858, time 20231.37ms, mfu 4.86%\n",
            "iter 71: loss 2.3906, time 20214.32ms, mfu 4.86%\n",
            "iter 72: loss 2.3976, time 20215.85ms, mfu 4.86%\n",
            "iter 73: loss 2.3997, time 20233.63ms, mfu 4.86%\n",
            "iter 74: loss 2.4182, time 20219.12ms, mfu 4.86%\n",
            "iter 75: loss 2.3961, time 20228.35ms, mfu 4.86%\n",
            "iter 76: loss 2.4102, time 20202.94ms, mfu 4.86%\n",
            "iter 77: loss 2.4309, time 20227.40ms, mfu 4.86%\n",
            "iter 78: loss 2.4241, time 20211.31ms, mfu 4.86%\n",
            "iter 79: loss 2.4031, time 20203.52ms, mfu 4.86%\n",
            "iter 80: loss 2.3739, time 20211.78ms, mfu 4.86%\n",
            "iter 81: loss 2.4019, time 20187.88ms, mfu 4.86%\n",
            "iter 82: loss 2.3939, time 20203.38ms, mfu 4.86%\n",
            "iter 83: loss 2.3825, time 20207.77ms, mfu 4.86%\n",
            "iter 84: loss 2.3887, time 20187.66ms, mfu 4.86%\n",
            "iter 85: loss 2.3980, time 20209.38ms, mfu 4.86%\n",
            "iter 86: loss 2.3875, time 20206.06ms, mfu 4.86%\n",
            "iter 87: loss 2.3204, time 20204.66ms, mfu 4.86%\n",
            "iter 88: loss 2.3844, time 20192.93ms, mfu 4.86%\n",
            "iter 89: loss 2.3802, time 20189.59ms, mfu 4.86%\n",
            "iter 90: loss 2.4094, time 20188.86ms, mfu 4.86%\n",
            "iter 91: loss 2.3884, time 20184.73ms, mfu 4.86%\n",
            "iter 92: loss 2.3770, time 20202.63ms, mfu 4.86%\n",
            "iter 93: loss 2.3750, time 20215.67ms, mfu 4.86%\n",
            "iter 94: loss 2.3826, time 20214.12ms, mfu 4.86%\n",
            "iter 95: loss 2.3718, time 20195.19ms, mfu 4.86%\n",
            "iter 96: loss 2.3934, time 20175.20ms, mfu 4.86%\n",
            "iter 97: loss 2.4222, time 20208.93ms, mfu 4.86%\n",
            "iter 98: loss 2.3818, time 20244.38ms, mfu 4.86%\n",
            "iter 99: loss 2.3635, time 20205.74ms, mfu 4.86%\n",
            "iter 100: loss 2.4203, time 20190.59ms, mfu 4.86%\n",
            "iter 101: loss 2.3784, time 20195.10ms, mfu 4.86%\n",
            "iter 102: loss 2.3643, time 20175.74ms, mfu 4.86%\n",
            "iter 103: loss 2.3438, time 20172.56ms, mfu 4.86%\n",
            "iter 104: loss 2.3816, time 20140.39ms, mfu 4.86%\n",
            "iter 105: loss 2.3672, time 20167.67ms, mfu 4.86%\n",
            "iter 106: loss 2.3674, time 20174.13ms, mfu 4.86%\n",
            "iter 107: loss 2.3544, time 20174.90ms, mfu 4.86%\n",
            "iter 108: loss 2.3686, time 20183.52ms, mfu 4.86%\n",
            "iter 109: loss 2.3716, time 20154.67ms, mfu 4.86%\n",
            "iter 110: loss 2.3373, time 20176.09ms, mfu 4.86%\n",
            "iter 111: loss 2.3714, time 20157.10ms, mfu 4.87%\n",
            "iter 112: loss 2.3750, time 20187.19ms, mfu 4.87%\n",
            "iter 113: loss 2.3752, time 20214.06ms, mfu 4.86%\n",
            "iter 114: loss 2.3541, time 20168.72ms, mfu 4.86%\n",
            "iter 115: loss 2.3436, time 20194.65ms, mfu 4.86%\n",
            "iter 116: loss 2.3561, time 20195.74ms, mfu 4.86%\n",
            "iter 117: loss 2.3661, time 20211.84ms, mfu 4.86%\n",
            "iter 118: loss 2.3543, time 20172.62ms, mfu 4.86%\n",
            "iter 119: loss 2.3526, time 20230.70ms, mfu 4.86%\n",
            "iter 120: loss 2.3674, time 20187.65ms, mfu 4.86%\n",
            "iter 121: loss 2.3629, time 20196.07ms, mfu 4.86%\n",
            "iter 122: loss 2.3544, time 20187.23ms, mfu 4.86%\n",
            "iter 123: loss 2.3567, time 20220.45ms, mfu 4.86%\n",
            "iter 124: loss 2.3673, time 20209.84ms, mfu 4.86%\n",
            "iter 125: loss 2.3351, time 20198.03ms, mfu 4.86%\n",
            "iter 126: loss 2.3704, time 20198.33ms, mfu 4.86%\n",
            "iter 127: loss 2.3069, time 20214.06ms, mfu 4.86%\n",
            "iter 128: loss 2.3315, time 20223.12ms, mfu 4.86%\n",
            "iter 129: loss 2.3349, time 20238.45ms, mfu 4.86%\n",
            "iter 130: loss 2.3860, time 20184.03ms, mfu 4.86%\n",
            "iter 131: loss 2.3781, time 20220.36ms, mfu 4.86%\n",
            "iter 132: loss 2.3574, time 20226.29ms, mfu 4.86%\n",
            "iter 133: loss 2.3314, time 20259.18ms, mfu 4.86%\n",
            "iter 134: loss 2.3723, time 20220.00ms, mfu 4.86%\n",
            "iter 135: loss 2.3681, time 20291.42ms, mfu 4.86%\n",
            "iter 136: loss 2.3750, time 20245.56ms, mfu 4.86%\n",
            "iter 137: loss 2.3578, time 20133.15ms, mfu 4.86%\n",
            "iter 138: loss 2.3431, time 20135.35ms, mfu 4.86%\n",
            "iter 139: loss 2.3297, time 20196.37ms, mfu 4.86%\n",
            "iter 140: loss 2.3516, time 20265.42ms, mfu 4.86%\n",
            "iter 141: loss 2.3570, time 20256.95ms, mfu 4.86%\n",
            "iter 142: loss 2.3259, time 20260.38ms, mfu 4.86%\n",
            "iter 143: loss 2.3234, time 20316.23ms, mfu 4.85%\n",
            "iter 144: loss 2.3390, time 20326.51ms, mfu 4.85%\n",
            "iter 145: loss 2.3289, time 20308.59ms, mfu 4.85%\n",
            "iter 146: loss 2.3557, time 20280.35ms, mfu 4.85%\n",
            "iter 147: loss 2.3179, time 20282.92ms, mfu 4.85%\n",
            "iter 148: loss 2.3339, time 20187.67ms, mfu 4.85%\n",
            "iter 149: loss 2.3031, time 20124.45ms, mfu 4.85%\n",
            "iter 150: loss 2.3350, time 20083.75ms, mfu 4.86%\n",
            "iter 151: loss 2.3026, time 20200.10ms, mfu 4.86%\n",
            "iter 152: loss 2.3072, time 20321.17ms, mfu 4.85%\n",
            "iter 153: loss 2.2964, time 20301.30ms, mfu 4.85%\n",
            "iter 154: loss 2.3135, time 20182.08ms, mfu 4.85%\n",
            "iter 155: loss 2.3275, time 20147.66ms, mfu 4.86%\n",
            "iter 156: loss 2.3442, time 20116.69ms, mfu 4.86%\n",
            "iter 157: loss 2.3152, time 20177.90ms, mfu 4.86%\n",
            "iter 158: loss 2.3119, time 20249.73ms, mfu 4.86%\n",
            "iter 159: loss 2.3135, time 20227.93ms, mfu 4.86%\n",
            "iter 160: loss 2.3134, time 20249.62ms, mfu 4.86%\n",
            "iter 161: loss 2.3666, time 20303.14ms, mfu 4.85%\n",
            "iter 162: loss 2.3177, time 20299.55ms, mfu 4.85%\n",
            "iter 163: loss 2.2979, time 20276.77ms, mfu 4.85%\n",
            "iter 164: loss 2.3182, time 20294.89ms, mfu 4.85%\n",
            "iter 165: loss 2.3019, time 20303.21ms, mfu 4.85%\n",
            "iter 166: loss 2.3268, time 20296.03ms, mfu 4.85%\n",
            "iter 167: loss 2.2789, time 20314.13ms, mfu 4.85%\n",
            "iter 168: loss 2.2913, time 20253.47ms, mfu 4.85%\n",
            "iter 169: loss 2.3098, time 20288.15ms, mfu 4.85%\n",
            "iter 170: loss 2.3140, time 20275.24ms, mfu 4.85%\n",
            "iter 171: loss 2.2654, time 20290.29ms, mfu 4.84%\n",
            "iter 172: loss 2.3106, time 20264.78ms, mfu 4.84%\n",
            "iter 173: loss 2.2939, time 20311.74ms, mfu 4.84%\n",
            "iter 174: loss 2.2778, time 20262.34ms, mfu 4.84%\n",
            "iter 175: loss 2.2762, time 20302.87ms, mfu 4.84%\n",
            "iter 176: loss 2.3011, time 20304.47ms, mfu 4.84%\n",
            "iter 177: loss 2.2766, time 20302.38ms, mfu 4.84%\n",
            "iter 178: loss 2.2818, time 20253.43ms, mfu 4.84%\n",
            "iter 179: loss 2.2606, time 20280.84ms, mfu 4.84%\n",
            "iter 180: loss 2.2480, time 20274.53ms, mfu 4.84%\n",
            "iter 181: loss 2.2721, time 20236.92ms, mfu 4.84%\n",
            "iter 182: loss 2.2747, time 20256.15ms, mfu 4.84%\n",
            "iter 183: loss 2.2577, time 20266.46ms, mfu 4.84%\n",
            "iter 184: loss 2.2455, time 20270.15ms, mfu 4.84%\n",
            "iter 185: loss 2.2448, time 20236.62ms, mfu 4.84%\n",
            "iter 186: loss 2.2413, time 20213.73ms, mfu 4.85%\n",
            "iter 187: loss 2.2867, time 20191.25ms, mfu 4.85%\n",
            "iter 188: loss 2.2703, time 20147.54ms, mfu 4.85%\n",
            "iter 189: loss 2.2909, time 20143.42ms, mfu 4.85%\n",
            "iter 190: loss 2.2369, time 20116.94ms, mfu 4.86%\n",
            "iter 191: loss 2.2811, time 20168.79ms, mfu 4.86%\n",
            "iter 192: loss 2.2394, time 20194.99ms, mfu 4.86%\n",
            "iter 193: loss 2.2364, time 20220.65ms, mfu 4.86%\n",
            "iter 194: loss 2.2614, time 20225.66ms, mfu 4.86%\n",
            "iter 195: loss 2.2448, time 20235.03ms, mfu 4.86%\n",
            "iter 196: loss 2.2611, time 20252.32ms, mfu 4.86%\n",
            "iter 197: loss 2.2707, time 20243.96ms, mfu 4.85%\n",
            "iter 198: loss 2.2324, time 20262.95ms, mfu 4.85%\n",
            "iter 199: loss 2.2358, time 20236.31ms, mfu 4.85%\n",
            "iter 200: loss 2.2456, time 20264.23ms, mfu 4.85%\n",
            "iter 201: loss 2.2283, time 20251.33ms, mfu 4.85%\n",
            "iter 202: loss 2.2637, time 20259.71ms, mfu 4.85%\n",
            "iter 203: loss 2.2106, time 20292.84ms, mfu 4.85%\n",
            "iter 204: loss 2.2225, time 20266.11ms, mfu 4.85%\n",
            "iter 205: loss 2.2685, time 20269.89ms, mfu 4.85%\n",
            "iter 206: loss 2.1983, time 20270.60ms, mfu 4.85%\n",
            "iter 207: loss 2.2519, time 20288.83ms, mfu 4.85%\n",
            "iter 208: loss 2.1971, time 20266.02ms, mfu 4.85%\n",
            "iter 209: loss 2.2476, time 20268.49ms, mfu 4.85%\n",
            "iter 210: loss 2.2973, time 20228.83ms, mfu 4.85%\n",
            "iter 211: loss 2.2279, time 20269.75ms, mfu 4.85%\n",
            "iter 212: loss 2.2510, time 20218.10ms, mfu 4.85%\n",
            "iter 213: loss 2.2387, time 20301.22ms, mfu 4.85%\n",
            "iter 214: loss 2.2867, time 20239.18ms, mfu 4.85%\n",
            "iter 215: loss 2.2363, time 20267.53ms, mfu 4.85%\n",
            "iter 216: loss 2.2170, time 20300.60ms, mfu 4.85%\n",
            "iter 217: loss 2.1371, time 20260.26ms, mfu 4.85%\n",
            "iter 218: loss 2.2198, time 20264.46ms, mfu 4.85%\n",
            "iter 219: loss 2.2419, time 20258.74ms, mfu 4.85%\n",
            "iter 220: loss 2.1736, time 20238.32ms, mfu 4.85%\n",
            "iter 221: loss 2.1776, time 20214.91ms, mfu 4.85%\n",
            "iter 222: loss 2.2066, time 20225.25ms, mfu 4.85%\n",
            "iter 223: loss 2.2334, time 20263.61ms, mfu 4.85%\n",
            "iter 224: loss 2.1864, time 20212.50ms, mfu 4.85%\n",
            "iter 225: loss 2.1841, time 20216.93ms, mfu 4.85%\n",
            "iter 226: loss 2.2145, time 20220.64ms, mfu 4.85%\n",
            "iter 227: loss 2.2162, time 20202.83ms, mfu 4.85%\n",
            "iter 228: loss 2.2539, time 20268.91ms, mfu 4.85%\n",
            "iter 229: loss 2.1918, time 20240.15ms, mfu 4.85%\n",
            "iter 230: loss 2.2188, time 20260.67ms, mfu 4.85%\n",
            "iter 231: loss 2.2349, time 20257.70ms, mfu 4.85%\n",
            "iter 232: loss 2.1625, time 20281.06ms, mfu 4.85%\n",
            "iter 233: loss 2.1786, time 20252.48ms, mfu 4.85%\n",
            "iter 234: loss 2.1805, time 20189.29ms, mfu 4.85%\n",
            "iter 235: loss 2.2107, time 20178.04ms, mfu 4.85%\n",
            "iter 236: loss 2.1743, time 20183.37ms, mfu 4.85%\n",
            "iter 237: loss 2.2179, time 20141.92ms, mfu 4.86%\n",
            "iter 238: loss 2.1985, time 20178.05ms, mfu 4.86%\n",
            "iter 239: loss 2.1742, time 20237.48ms, mfu 4.86%\n",
            "iter 240: loss 2.2340, time 20263.60ms, mfu 4.85%\n",
            "iter 241: loss 2.1859, time 20296.22ms, mfu 4.85%\n",
            "iter 242: loss 2.1848, time 20205.82ms, mfu 4.85%\n",
            "iter 243: loss 2.1923, time 20199.47ms, mfu 4.85%\n",
            "iter 244: loss 2.1717, time 20164.05ms, mfu 4.86%\n",
            "iter 245: loss 2.1624, time 20156.71ms, mfu 4.86%\n",
            "iter 246: loss 2.1972, time 20159.57ms, mfu 4.86%\n",
            "iter 247: loss 2.2200, time 20175.05ms, mfu 4.86%\n",
            "iter 248: loss 2.1672, time 20165.09ms, mfu 4.86%\n",
            "iter 249: loss 2.1922, time 20185.94ms, mfu 4.86%\n",
            "iter 250: loss 2.1875, time 20138.44ms, mfu 4.86%\n",
            "iter 251: loss 2.1982, time 20158.74ms, mfu 4.86%\n",
            "iter 252: loss 2.2448, time 20178.78ms, mfu 4.86%\n",
            "iter 253: loss 2.2060, time 20178.67ms, mfu 4.86%\n",
            "iter 254: loss 2.2124, time 20150.35ms, mfu 4.86%\n",
            "iter 255: loss 2.2234, time 20185.86ms, mfu 4.86%\n",
            "iter 256: loss 2.1746, time 20187.36ms, mfu 4.86%\n",
            "iter 257: loss 2.1745, time 20238.64ms, mfu 4.86%\n",
            "iter 258: loss 2.1951, time 20309.14ms, mfu 4.86%\n",
            "iter 259: loss 2.1988, time 20245.05ms, mfu 4.86%\n",
            "iter 260: loss 2.1757, time 20145.49ms, mfu 4.86%\n",
            "iter 261: loss 2.2052, time 20140.06ms, mfu 4.86%\n",
            "iter 262: loss 2.2026, time 20259.30ms, mfu 4.86%\n",
            "iter 263: loss 2.1706, time 20278.59ms, mfu 4.86%\n",
            "iter 264: loss 2.1195, time 20239.89ms, mfu 4.86%\n",
            "iter 265: loss 2.2391, time 20265.94ms, mfu 4.86%\n",
            "iter 266: loss 2.1805, time 20267.73ms, mfu 4.86%\n",
            "iter 267: loss 2.1565, time 20239.94ms, mfu 4.85%\n",
            "iter 268: loss 2.1246, time 20266.38ms, mfu 4.85%\n",
            "iter 269: loss 2.1435, time 20241.38ms, mfu 4.85%\n",
            "iter 270: loss 2.1785, time 20237.95ms, mfu 4.85%\n",
            "iter 271: loss 2.1560, time 20248.50ms, mfu 4.85%\n",
            "iter 272: loss 2.2279, time 20259.68ms, mfu 4.85%\n",
            "iter 273: loss 2.2469, time 20252.01ms, mfu 4.85%\n",
            "iter 274: loss 2.1447, time 20255.30ms, mfu 4.85%\n",
            "iter 275: loss 2.1736, time 20221.37ms, mfu 4.85%\n",
            "iter 276: loss 2.1675, time 20238.66ms, mfu 4.85%\n",
            "iter 277: loss 2.1202, time 20230.28ms, mfu 4.85%\n",
            "iter 278: loss 2.1211, time 20190.17ms, mfu 4.85%\n",
            "iter 279: loss 2.2035, time 20215.02ms, mfu 4.85%\n",
            "iter 280: loss 2.1295, time 20211.31ms, mfu 4.85%\n",
            "iter 281: loss 2.1464, time 20188.72ms, mfu 4.85%\n",
            "iter 282: loss 2.1111, time 20217.05ms, mfu 4.85%\n",
            "iter 283: loss 2.2004, time 20220.82ms, mfu 4.85%\n",
            "iter 284: loss 2.1572, time 20225.89ms, mfu 4.85%\n",
            "iter 285: loss 2.1510, time 20190.55ms, mfu 4.86%\n",
            "iter 286: loss 2.1732, time 20206.88ms, mfu 4.86%\n",
            "iter 287: loss 2.1332, time 20169.53ms, mfu 4.86%\n",
            "iter 288: loss 2.1619, time 20223.87ms, mfu 4.86%\n",
            "iter 289: loss 2.1839, time 20244.38ms, mfu 4.86%\n",
            "iter 290: loss 2.1038, time 20302.66ms, mfu 4.85%\n",
            "iter 291: loss 2.1164, time 20196.16ms, mfu 4.85%\n",
            "iter 292: loss 2.1162, time 20180.26ms, mfu 4.86%\n",
            "iter 293: loss 2.1728, time 20147.60ms, mfu 4.86%\n",
            "iter 294: loss 2.1191, time 20094.58ms, mfu 4.86%\n",
            "iter 295: loss 2.0816, time 20105.37ms, mfu 4.86%\n",
            "iter 296: loss 2.1185, time 20112.22ms, mfu 4.86%\n",
            "iter 297: loss 2.1219, time 20106.44ms, mfu 4.87%\n",
            "iter 298: loss 2.1287, time 20167.90ms, mfu 4.87%\n",
            "iter 299: loss 2.1462, time 20122.48ms, mfu 4.87%\n",
            "iter 300: loss 2.0796, time 20135.61ms, mfu 4.87%\n",
            "iter 301: loss 2.1149, time 20176.82ms, mfu 4.87%\n",
            "iter 302: loss 2.0521, time 20157.65ms, mfu 4.87%\n",
            "iter 303: loss 2.1617, time 20221.35ms, mfu 4.87%\n",
            "iter 304: loss 2.0948, time 20194.61ms, mfu 4.87%\n",
            "iter 305: loss 2.1141, time 20150.49ms, mfu 4.87%\n",
            "iter 306: loss 2.0596, time 20198.67ms, mfu 4.87%\n",
            "iter 307: loss 2.1479, time 20168.35ms, mfu 4.87%\n",
            "iter 308: loss 2.1012, time 20191.64ms, mfu 4.87%\n",
            "iter 309: loss 2.0759, time 20174.34ms, mfu 4.87%\n",
            "iter 310: loss 2.1058, time 20175.38ms, mfu 4.87%\n",
            "iter 311: loss 2.0360, time 20168.77ms, mfu 4.87%\n",
            "iter 312: loss 2.0573, time 20179.75ms, mfu 4.87%\n",
            "iter 313: loss 2.1156, time 20218.01ms, mfu 4.87%\n",
            "iter 314: loss 2.1362, time 20227.22ms, mfu 4.86%\n",
            "iter 315: loss 2.0259, time 20280.28ms, mfu 4.86%\n",
            "iter 316: loss 2.0709, time 20302.70ms, mfu 4.86%\n",
            "iter 317: loss 2.0490, time 20293.17ms, mfu 4.86%\n",
            "iter 318: loss 2.0449, time 20265.75ms, mfu 4.86%\n",
            "iter 319: loss 2.0681, time 20231.94ms, mfu 4.86%\n",
            "iter 320: loss 2.1089, time 20195.28ms, mfu 4.86%\n",
            "iter 321: loss 2.1102, time 20100.66ms, mfu 4.86%\n",
            "iter 322: loss 2.0937, time 20133.30ms, mfu 4.86%\n",
            "iter 323: loss 2.0839, time 20202.39ms, mfu 4.86%\n",
            "iter 324: loss 2.0736, time 20229.39ms, mfu 4.86%\n",
            "iter 325: loss 2.0569, time 20117.41ms, mfu 4.86%\n",
            "iter 326: loss 2.0406, time 20174.42ms, mfu 4.86%\n",
            "iter 327: loss 2.0925, time 20160.41ms, mfu 4.86%\n",
            "iter 328: loss 2.0364, time 20156.49ms, mfu 4.86%\n",
            "iter 329: loss 2.0378, time 20201.83ms, mfu 4.86%\n",
            "iter 330: loss 2.0219, time 20173.52ms, mfu 4.86%\n",
            "iter 331: loss 2.0149, time 20176.34ms, mfu 4.86%\n",
            "iter 332: loss 2.0450, time 20204.40ms, mfu 4.86%\n",
            "iter 333: loss 2.1089, time 20200.41ms, mfu 4.86%\n",
            "iter 334: loss 2.0315, time 20175.83ms, mfu 4.86%\n",
            "iter 335: loss 2.1003, time 20199.29ms, mfu 4.86%\n",
            "iter 336: loss 2.0046, time 20221.85ms, mfu 4.86%\n",
            "iter 337: loss 2.0479, time 20190.75ms, mfu 4.86%\n",
            "iter 338: loss 1.9894, time 20207.57ms, mfu 4.86%\n",
            "iter 339: loss 2.0855, time 20205.18ms, mfu 4.86%\n",
            "iter 340: loss 2.0538, time 20243.14ms, mfu 4.86%\n",
            "iter 341: loss 2.0683, time 20206.94ms, mfu 4.86%\n",
            "iter 342: loss 2.0149, time 20200.57ms, mfu 4.86%\n",
            "iter 343: loss 2.0497, time 20228.81ms, mfu 4.86%\n",
            "iter 344: loss 2.0522, time 20218.58ms, mfu 4.86%\n",
            "iter 345: loss 2.0478, time 20219.06ms, mfu 4.86%\n",
            "iter 346: loss 2.0232, time 20193.54ms, mfu 4.86%\n",
            "iter 347: loss 2.0256, time 20193.62ms, mfu 4.86%\n",
            "iter 348: loss 1.9705, time 20192.56ms, mfu 4.86%\n",
            "iter 349: loss 1.9670, time 20204.84ms, mfu 4.86%\n",
            "iter 350: loss 2.0649, time 20226.40ms, mfu 4.86%\n",
            "iter 351: loss 1.9681, time 20190.39ms, mfu 4.86%\n",
            "iter 352: loss 2.0129, time 20204.65ms, mfu 4.86%\n",
            "iter 353: loss 1.9708, time 20191.29ms, mfu 4.86%\n",
            "iter 354: loss 1.9847, time 20153.01ms, mfu 4.86%\n",
            "iter 355: loss 1.9881, time 20129.46ms, mfu 4.86%\n",
            "iter 356: loss 1.9609, time 20116.78ms, mfu 4.86%\n",
            "iter 357: loss 1.9457, time 20147.78ms, mfu 4.87%\n",
            "iter 358: loss 1.9701, time 20193.00ms, mfu 4.87%\n",
            "iter 359: loss 2.0328, time 20194.25ms, mfu 4.86%\n",
            "iter 360: loss 1.9404, time 20195.97ms, mfu 4.86%\n",
            "iter 361: loss 1.9466, time 20189.77ms, mfu 4.86%\n",
            "iter 362: loss 1.9402, time 20221.05ms, mfu 4.86%\n",
            "iter 363: loss 2.0214, time 20224.61ms, mfu 4.86%\n",
            "iter 364: loss 2.0110, time 20163.63ms, mfu 4.86%\n",
            "iter 365: loss 1.9918, time 20123.37ms, mfu 4.86%\n",
            "iter 366: loss 1.9685, time 20159.24ms, mfu 4.87%\n",
            "iter 367: loss 1.9481, time 20163.23ms, mfu 4.87%\n",
            "iter 368: loss 1.9119, time 20198.82ms, mfu 4.87%\n",
            "iter 369: loss 1.9541, time 20173.09ms, mfu 4.87%\n",
            "iter 370: loss 1.9628, time 20229.41ms, mfu 4.86%\n",
            "iter 371: loss 1.9732, time 20174.93ms, mfu 4.86%\n",
            "iter 372: loss 1.9995, time 20194.67ms, mfu 4.86%\n",
            "iter 373: loss 1.9821, time 20240.34ms, mfu 4.86%\n",
            "iter 374: loss 1.9651, time 20200.07ms, mfu 4.86%\n",
            "iter 375: loss 1.9958, time 20200.95ms, mfu 4.86%\n",
            "iter 376: loss 1.9296, time 20199.58ms, mfu 4.86%\n",
            "iter 377: loss 1.9499, time 20188.88ms, mfu 4.86%\n",
            "iter 378: loss 1.9762, time 20181.45ms, mfu 4.86%\n",
            "iter 379: loss 1.8824, time 20193.45ms, mfu 4.86%\n",
            "iter 380: loss 1.9851, time 20191.49ms, mfu 4.86%\n",
            "iter 381: loss 1.8880, time 20207.88ms, mfu 4.86%\n",
            "iter 382: loss 2.0152, time 20204.13ms, mfu 4.86%\n",
            "iter 383: loss 1.8945, time 20195.19ms, mfu 4.86%\n",
            "iter 384: loss 1.9609, time 20188.30ms, mfu 4.86%\n",
            "iter 385: loss 1.9144, time 20230.37ms, mfu 4.86%\n",
            "iter 386: loss 1.9168, time 20204.84ms, mfu 4.86%\n",
            "iter 387: loss 1.9421, time 20219.19ms, mfu 4.86%\n",
            "iter 388: loss 1.9243, time 20203.76ms, mfu 4.86%\n",
            "iter 389: loss 1.9249, time 20144.26ms, mfu 4.86%\n",
            "iter 390: loss 1.9200, time 20105.38ms, mfu 4.86%\n",
            "iter 391: loss 1.9157, time 20136.81ms, mfu 4.87%\n",
            "iter 392: loss 1.8917, time 20255.24ms, mfu 4.86%\n",
            "iter 393: loss 1.8826, time 20245.28ms, mfu 4.86%\n",
            "iter 394: loss 1.8836, time 20175.08ms, mfu 4.86%\n",
            "iter 395: loss 1.9187, time 20134.81ms, mfu 4.86%\n",
            "iter 396: loss 2.0015, time 20093.57ms, mfu 4.87%\n",
            "iter 397: loss 1.9268, time 20137.46ms, mfu 4.87%\n",
            "iter 398: loss 1.8765, time 20130.81ms, mfu 4.87%\n",
            "iter 399: loss 1.8950, time 20172.14ms, mfu 4.87%\n",
            "iter 400: loss 1.9657, time 20182.63ms, mfu 4.87%\n",
            "iter 401: loss 1.8506, time 20253.51ms, mfu 4.87%\n",
            "iter 402: loss 1.9211, time 20236.04ms, mfu 4.86%\n",
            "iter 403: loss 1.9296, time 20248.34ms, mfu 4.86%\n",
            "iter 404: loss 1.9268, time 20273.93ms, mfu 4.86%\n",
            "iter 405: loss 1.8491, time 20319.55ms, mfu 4.86%\n",
            "iter 406: loss 1.8537, time 20303.79ms, mfu 4.86%\n",
            "iter 407: loss 1.8848, time 20258.60ms, mfu 4.85%\n",
            "iter 408: loss 1.8390, time 20218.53ms, mfu 4.85%\n",
            "iter 409: loss 1.9105, time 20203.77ms, mfu 4.86%\n",
            "iter 410: loss 1.9176, time 20260.90ms, mfu 4.85%\n",
            "iter 411: loss 1.8932, time 20237.79ms, mfu 4.85%\n",
            "iter 412: loss 1.8616, time 20234.83ms, mfu 4.85%\n",
            "iter 413: loss 1.8756, time 20191.20ms, mfu 4.85%\n",
            "iter 414: loss 1.9124, time 20214.12ms, mfu 4.86%\n",
            "iter 415: loss 1.8327, time 20256.10ms, mfu 4.85%\n",
            "iter 416: loss 1.8294, time 20271.23ms, mfu 4.85%\n",
            "iter 417: loss 1.8407, time 20241.63ms, mfu 4.85%\n",
            "iter 418: loss 1.8879, time 20252.52ms, mfu 4.85%\n",
            "iter 419: loss 1.8604, time 20267.89ms, mfu 4.85%\n",
            "iter 420: loss 1.9337, time 20282.23ms, mfu 4.85%\n",
            "iter 421: loss 1.8782, time 20276.92ms, mfu 4.85%\n",
            "iter 422: loss 1.7984, time 20300.20ms, mfu 4.85%\n",
            "iter 423: loss 1.8312, time 20262.36ms, mfu 4.85%\n",
            "iter 424: loss 1.8673, time 20199.77ms, mfu 4.85%\n",
            "iter 425: loss 1.8793, time 20183.27ms, mfu 4.85%\n",
            "iter 426: loss 1.8519, time 20165.42ms, mfu 4.85%\n",
            "iter 427: loss 1.7981, time 20143.25ms, mfu 4.85%\n",
            "iter 428: loss 1.8335, time 20142.87ms, mfu 4.86%\n",
            "iter 429: loss 1.8194, time 20149.23ms, mfu 4.86%\n",
            "iter 430: loss 1.8781, time 20176.31ms, mfu 4.86%\n",
            "iter 431: loss 1.8373, time 20147.86ms, mfu 4.86%\n",
            "iter 432: loss 1.8356, time 20148.29ms, mfu 4.86%\n",
            "iter 433: loss 1.8097, time 20176.93ms, mfu 4.86%\n",
            "iter 434: loss 1.9114, time 20144.51ms, mfu 4.86%\n",
            "iter 435: loss 1.7755, time 20158.22ms, mfu 4.86%\n",
            "iter 436: loss 1.7864, time 20089.00ms, mfu 4.87%\n",
            "iter 437: loss 1.8133, time 20098.04ms, mfu 4.87%\n",
            "iter 438: loss 1.8142, time 20112.37ms, mfu 4.87%\n",
            "iter 439: loss 1.8839, time 20097.91ms, mfu 4.87%\n",
            "iter 440: loss 1.7679, time 20129.63ms, mfu 4.87%\n",
            "iter 441: loss 1.8615, time 20192.45ms, mfu 4.87%\n",
            "iter 442: loss 1.8286, time 20222.41ms, mfu 4.87%\n",
            "iter 443: loss 1.7840, time 20218.82ms, mfu 4.87%\n",
            "iter 444: loss 1.8983, time 20276.49ms, mfu 4.87%\n",
            "iter 445: loss 1.7991, time 20299.95ms, mfu 4.86%\n",
            "iter 446: loss 1.8141, time 20248.24ms, mfu 4.86%\n",
            "iter 447: loss 1.7364, time 20243.08ms, mfu 4.86%\n",
            "iter 448: loss 1.8016, time 20203.62ms, mfu 4.86%\n",
            "iter 449: loss 1.7614, time 20248.68ms, mfu 4.86%\n",
            "iter 450: loss 1.8423, time 20198.23ms, mfu 4.86%\n",
            "iter 451: loss 1.8668, time 20206.43ms, mfu 4.86%\n",
            "iter 452: loss 1.8254, time 20175.90ms, mfu 4.86%\n",
            "iter 453: loss 1.7846, time 20106.60ms, mfu 4.86%\n",
            "iter 454: loss 1.7730, time 20111.67ms, mfu 4.86%\n",
            "iter 455: loss 1.7759, time 20155.01ms, mfu 4.86%\n",
            "iter 456: loss 1.8452, time 20182.83ms, mfu 4.86%\n",
            "iter 457: loss 1.8373, time 20189.90ms, mfu 4.86%\n",
            "iter 458: loss 1.8586, time 20196.48ms, mfu 4.86%\n",
            "iter 459: loss 1.7774, time 20208.61ms, mfu 4.86%\n",
            "iter 460: loss 1.7966, time 20216.54ms, mfu 4.86%\n",
            "iter 461: loss 1.7221, time 20234.37ms, mfu 4.86%\n",
            "iter 462: loss 1.7599, time 20220.06ms, mfu 4.86%\n",
            "iter 463: loss 1.7736, time 20212.05ms, mfu 4.86%\n",
            "iter 464: loss 1.7782, time 20209.99ms, mfu 4.86%\n",
            "iter 465: loss 1.7366, time 20238.78ms, mfu 4.86%\n",
            "iter 466: loss 1.7921, time 20251.99ms, mfu 4.86%\n",
            "iter 467: loss 1.7531, time 20238.61ms, mfu 4.86%\n",
            "iter 468: loss 1.7350, time 20230.53ms, mfu 4.86%\n",
            "iter 469: loss 1.7918, time 20244.60ms, mfu 4.86%\n",
            "iter 470: loss 1.7403, time 20255.74ms, mfu 4.86%\n",
            "iter 471: loss 1.7589, time 20242.43ms, mfu 4.86%\n",
            "iter 472: loss 1.7390, time 20238.95ms, mfu 4.85%\n",
            "iter 473: loss 1.7171, time 20263.11ms, mfu 4.85%\n",
            "iter 474: loss 1.7620, time 20260.91ms, mfu 4.85%\n",
            "iter 475: loss 1.7500, time 20233.22ms, mfu 4.85%\n",
            "iter 476: loss 1.7087, time 20265.90ms, mfu 4.85%\n",
            "iter 477: loss 1.6905, time 20244.29ms, mfu 4.85%\n",
            "iter 478: loss 1.7743, time 20192.69ms, mfu 4.85%\n",
            "iter 479: loss 1.7078, time 20205.36ms, mfu 4.85%\n",
            "iter 480: loss 1.7280, time 20173.74ms, mfu 4.85%\n",
            "iter 481: loss 1.7063, time 20153.32ms, mfu 4.86%\n",
            "iter 482: loss 1.7509, time 20131.52ms, mfu 4.86%\n",
            "iter 483: loss 1.7328, time 20152.18ms, mfu 4.86%\n",
            "iter 484: loss 1.7927, time 20144.29ms, mfu 4.86%\n",
            "iter 485: loss 1.7173, time 20100.29ms, mfu 4.86%\n",
            "iter 486: loss 1.7611, time 20105.75ms, mfu 4.87%\n",
            "iter 487: loss 1.7669, time 20148.65ms, mfu 4.87%\n",
            "iter 488: loss 1.7682, time 20153.28ms, mfu 4.87%\n",
            "iter 489: loss 1.7189, time 20193.69ms, mfu 4.87%\n",
            "iter 490: loss 1.7575, time 20174.81ms, mfu 4.87%\n",
            "iter 491: loss 1.7662, time 20206.84ms, mfu 4.87%\n",
            "iter 492: loss 1.7010, time 20201.91ms, mfu 4.87%\n",
            "iter 493: loss 1.7084, time 20200.29ms, mfu 4.86%\n",
            "iter 494: loss 1.6478, time 20210.88ms, mfu 4.86%\n",
            "iter 495: loss 1.7427, time 20254.92ms, mfu 4.86%\n",
            "iter 496: loss 1.7260, time 20235.23ms, mfu 4.86%\n",
            "iter 497: loss 1.7663, time 20257.39ms, mfu 4.86%\n",
            "iter 498: loss 1.6774, time 20230.68ms, mfu 4.86%\n",
            "iter 499: loss 1.7112, time 20241.82ms, mfu 4.86%\n",
            "iter 500: loss 1.7133, time 20194.71ms, mfu 4.86%\n",
            "iter 501: loss 1.6542, time 20196.63ms, mfu 4.86%\n",
            "iter 502: loss 1.6500, time 20163.95ms, mfu 4.86%\n",
            "iter 503: loss 1.6468, time 20198.51ms, mfu 4.86%\n",
            "iter 504: loss 1.6892, time 20212.45ms, mfu 4.86%\n",
            "iter 505: loss 1.7080, time 20249.27ms, mfu 4.86%\n",
            "iter 506: loss 1.7336, time 20231.55ms, mfu 4.86%\n",
            "iter 507: loss 1.6996, time 20221.54ms, mfu 4.86%\n",
            "iter 508: loss 1.6856, time 20230.12ms, mfu 4.86%\n",
            "iter 509: loss 1.6865, time 20259.33ms, mfu 4.86%\n",
            "iter 510: loss 1.7434, time 20220.00ms, mfu 4.86%\n",
            "iter 511: loss 1.7098, time 20224.62ms, mfu 4.86%\n",
            "iter 512: loss 1.6923, time 20191.49ms, mfu 4.86%\n",
            "iter 513: loss 1.7025, time 20184.24ms, mfu 4.86%\n",
            "iter 514: loss 1.6513, time 20169.57ms, mfu 4.86%\n",
            "iter 515: loss 1.6882, time 20140.75ms, mfu 4.86%\n",
            "iter 516: loss 1.6548, time 20147.45ms, mfu 4.86%\n",
            "iter 517: loss 1.5778, time 20102.92ms, mfu 4.86%\n",
            "iter 518: loss 1.6049, time 20109.91ms, mfu 4.87%\n",
            "iter 519: loss 1.6372, time 20109.89ms, mfu 4.87%\n",
            "iter 520: loss 1.6925, time 20114.25ms, mfu 4.87%\n",
            "iter 521: loss 1.6476, time 20109.65ms, mfu 4.87%\n",
            "iter 522: loss 1.6819, time 20102.79ms, mfu 4.87%\n",
            "iter 523: loss 1.6624, time 20081.97ms, mfu 4.87%\n",
            "iter 524: loss 1.6659, time 20110.60ms, mfu 4.87%\n",
            "iter 525: loss 1.6116, time 20110.49ms, mfu 4.87%\n",
            "iter 526: loss 1.6677, time 20154.18ms, mfu 4.87%\n",
            "iter 527: loss 1.6384, time 20144.48ms, mfu 4.87%\n",
            "iter 528: loss 1.6700, time 20191.98ms, mfu 4.87%\n",
            "iter 529: loss 1.7057, time 20228.19ms, mfu 4.87%\n",
            "iter 530: loss 1.6561, time 20241.23ms, mfu 4.87%\n",
            "iter 531: loss 1.6807, time 20254.55ms, mfu 4.87%\n",
            "iter 532: loss 1.6295, time 20272.63ms, mfu 4.86%\n",
            "iter 533: loss 1.6341, time 20213.83ms, mfu 4.86%\n",
            "iter 534: loss 1.6473, time 20191.30ms, mfu 4.86%\n",
            "iter 535: loss 1.6930, time 20195.22ms, mfu 4.86%\n",
            "iter 536: loss 1.6272, time 20205.14ms, mfu 4.86%\n",
            "iter 537: loss 1.6708, time 20161.34ms, mfu 4.86%\n",
            "iter 538: loss 1.6159, time 20145.61ms, mfu 4.86%\n",
            "iter 539: loss 1.6514, time 20110.19ms, mfu 4.87%\n",
            "iter 540: loss 1.7269, time 20102.22ms, mfu 4.87%\n",
            "iter 541: loss 1.6222, time 20107.99ms, mfu 4.87%\n",
            "iter 542: loss 1.6340, time 20158.36ms, mfu 4.87%\n",
            "iter 543: loss 1.5936, time 20177.16ms, mfu 4.87%\n",
            "iter 544: loss 1.6843, time 20235.84ms, mfu 4.87%\n",
            "iter 545: loss 1.6208, time 20254.68ms, mfu 4.87%\n",
            "iter 546: loss 1.6320, time 20235.51ms, mfu 4.86%\n",
            "iter 547: loss 1.6527, time 20277.75ms, mfu 4.86%\n",
            "iter 548: loss 1.5253, time 20274.66ms, mfu 4.86%\n",
            "iter 549: loss 1.6624, time 20289.34ms, mfu 4.86%\n",
            "iter 550: loss 1.6169, time 20249.89ms, mfu 4.86%\n",
            "iter 551: loss 1.6246, time 20229.47ms, mfu 4.86%\n",
            "iter 552: loss 1.5741, time 20249.63ms, mfu 4.86%\n",
            "iter 553: loss 1.5904, time 20199.15ms, mfu 4.86%\n",
            "iter 554: loss 1.6279, time 20187.40ms, mfu 4.86%\n",
            "iter 555: loss 1.5544, time 20173.20ms, mfu 4.86%\n",
            "iter 556: loss 1.5807, time 20161.68ms, mfu 4.86%\n",
            "iter 557: loss 1.6527, time 20165.78ms, mfu 4.86%\n",
            "iter 558: loss 1.6265, time 20173.32ms, mfu 4.86%\n",
            "iter 559: loss 1.6320, time 20182.71ms, mfu 4.86%\n",
            "iter 560: loss 1.6274, time 20202.83ms, mfu 4.86%\n",
            "iter 561: loss 1.5980, time 20234.72ms, mfu 4.86%\n",
            "iter 562: loss 1.6134, time 20224.90ms, mfu 4.86%\n",
            "iter 563: loss 1.6104, time 20276.49ms, mfu 4.86%\n",
            "iter 564: loss 1.5723, time 20254.28ms, mfu 4.86%\n",
            "iter 565: loss 1.6332, time 20279.01ms, mfu 4.86%\n",
            "iter 566: loss 1.5579, time 20296.55ms, mfu 4.85%\n",
            "iter 567: loss 1.5705, time 20270.28ms, mfu 4.85%\n",
            "iter 568: loss 1.5526, time 20264.96ms, mfu 4.85%\n",
            "iter 569: loss 1.5929, time 20272.42ms, mfu 4.85%\n",
            "iter 570: loss 1.6734, time 20206.27ms, mfu 4.85%\n",
            "iter 571: loss 1.6275, time 20163.07ms, mfu 4.85%\n",
            "iter 572: loss 1.5543, time 20160.14ms, mfu 4.86%\n",
            "iter 573: loss 1.5197, time 20161.03ms, mfu 4.86%\n",
            "iter 574: loss 1.6201, time 20131.27ms, mfu 4.86%\n",
            "iter 575: loss 1.5554, time 20172.67ms, mfu 4.86%\n",
            "iter 576: loss 1.5647, time 20159.84ms, mfu 4.86%\n",
            "iter 577: loss 1.5124, time 20128.26ms, mfu 4.86%\n",
            "iter 578: loss 1.5076, time 20211.97ms, mfu 4.86%\n",
            "iter 579: loss 1.5188, time 20243.40ms, mfu 4.86%\n",
            "iter 580: loss 1.5329, time 20235.78ms, mfu 4.86%\n",
            "iter 581: loss 1.5157, time 20262.99ms, mfu 4.86%\n",
            "iter 582: loss 1.5194, time 20240.79ms, mfu 4.86%\n",
            "iter 583: loss 1.5452, time 20271.13ms, mfu 4.86%\n",
            "iter 584: loss 1.6266, time 20256.85ms, mfu 4.86%\n",
            "iter 585: loss 1.5271, time 20272.35ms, mfu 4.85%\n",
            "iter 586: loss 1.4729, time 20305.87ms, mfu 4.85%\n",
            "iter 587: loss 1.6577, time 20223.75ms, mfu 4.85%\n",
            "iter 588: loss 1.5943, time 20217.28ms, mfu 4.85%\n",
            "iter 589: loss 1.6150, time 20188.29ms, mfu 4.85%\n",
            "iter 590: loss 1.5655, time 20188.53ms, mfu 4.85%\n",
            "iter 591: loss 1.5833, time 20131.80ms, mfu 4.86%\n",
            "iter 592: loss 1.6047, time 20152.41ms, mfu 4.86%\n",
            "iter 593: loss 1.5470, time 20172.79ms, mfu 4.86%\n",
            "iter 594: loss 1.5882, time 20211.33ms, mfu 4.86%\n",
            "iter 595: loss 1.6363, time 20216.30ms, mfu 4.86%\n",
            "iter 596: loss 1.5172, time 20278.82ms, mfu 4.86%\n",
            "iter 597: loss 1.4893, time 20214.26ms, mfu 4.86%\n",
            "iter 598: loss 1.6186, time 20248.05ms, mfu 4.86%\n",
            "iter 599: loss 1.6014, time 20224.24ms, mfu 4.86%\n",
            "iter 600: loss 1.4895, time 20289.22ms, mfu 4.85%\n",
            "iter 601: loss 1.5766, time 20270.35ms, mfu 4.85%\n",
            "iter 602: loss 1.5251, time 20303.82ms, mfu 4.85%\n",
            "iter 603: loss 1.5775, time 20195.86ms, mfu 4.85%\n",
            "iter 604: loss 1.5209, time 20217.05ms, mfu 4.85%\n",
            "iter 605: loss 1.5314, time 20186.68ms, mfu 4.85%\n",
            "iter 606: loss 1.5957, time 20164.72ms, mfu 4.86%\n",
            "iter 607: loss 1.5520, time 20194.74ms, mfu 4.86%\n",
            "iter 608: loss 1.5714, time 20157.58ms, mfu 4.86%\n",
            "iter 609: loss 1.5038, time 20171.62ms, mfu 4.86%\n",
            "iter 610: loss 1.5950, time 20169.00ms, mfu 4.86%\n",
            "iter 611: loss 1.5136, time 20116.68ms, mfu 4.86%\n",
            "iter 612: loss 1.5607, time 20138.93ms, mfu 4.86%\n",
            "iter 613: loss 1.5279, time 20167.02ms, mfu 4.86%\n",
            "iter 614: loss 1.5290, time 20208.68ms, mfu 4.86%\n",
            "iter 615: loss 1.5524, time 20226.63ms, mfu 4.86%\n",
            "iter 616: loss 1.5896, time 20208.05ms, mfu 4.86%\n",
            "iter 617: loss 1.5021, time 20217.59ms, mfu 4.86%\n",
            "iter 618: loss 1.5085, time 20255.68ms, mfu 4.86%\n",
            "iter 619: loss 1.6304, time 20235.69ms, mfu 4.86%\n",
            "iter 620: loss 1.4863, time 20232.48ms, mfu 4.86%\n",
            "iter 621: loss 1.5360, time 20218.34ms, mfu 4.86%\n",
            "iter 622: loss 1.5289, time 20233.56ms, mfu 4.86%\n",
            "iter 623: loss 1.5793, time 20287.07ms, mfu 4.86%\n",
            "iter 624: loss 1.4451, time 20273.52ms, mfu 4.85%\n",
            "iter 625: loss 1.4901, time 20239.92ms, mfu 4.85%\n",
            "iter 626: loss 1.5173, time 20202.81ms, mfu 4.85%\n",
            "iter 627: loss 1.4997, time 20175.57ms, mfu 4.86%\n",
            "iter 628: loss 1.4831, time 20186.05ms, mfu 4.86%\n",
            "iter 629: loss 1.4553, time 20141.13ms, mfu 4.86%\n",
            "iter 630: loss 1.4711, time 20139.10ms, mfu 4.86%\n",
            "iter 631: loss 1.5153, time 20137.06ms, mfu 4.86%\n",
            "iter 632: loss 1.5415, time 20134.70ms, mfu 4.86%\n",
            "iter 633: loss 1.5985, time 20129.10ms, mfu 4.86%\n",
            "iter 634: loss 1.5073, time 20143.60ms, mfu 4.87%\n",
            "iter 635: loss 1.5455, time 20141.19ms, mfu 4.87%\n",
            "iter 636: loss 1.5207, time 20111.52ms, mfu 4.87%\n",
            "iter 637: loss 1.4687, time 20121.49ms, mfu 4.87%\n",
            "iter 638: loss 1.4759, time 20208.17ms, mfu 4.87%\n",
            "iter 639: loss 1.4537, time 20204.83ms, mfu 4.87%\n",
            "iter 640: loss 1.5095, time 20225.89ms, mfu 4.87%\n",
            "iter 641: loss 1.4733, time 20236.65ms, mfu 4.86%\n",
            "iter 642: loss 1.5185, time 20239.05ms, mfu 4.86%\n",
            "iter 643: loss 1.4728, time 20269.59ms, mfu 4.86%\n",
            "iter 644: loss 1.4459, time 20265.51ms, mfu 4.86%\n",
            "iter 645: loss 1.4299, time 20255.81ms, mfu 4.86%\n",
            "iter 646: loss 1.5017, time 20245.44ms, mfu 4.86%\n",
            "iter 647: loss 1.5066, time 20277.95ms, mfu 4.86%\n",
            "iter 648: loss 1.4995, time 20249.11ms, mfu 4.86%\n",
            "iter 649: loss 1.4874, time 20286.26ms, mfu 4.85%\n",
            "iter 650: loss 1.4178, time 20264.88ms, mfu 4.85%\n",
            "iter 651: loss 1.4611, time 20215.02ms, mfu 4.85%\n",
            "iter 652: loss 1.4899, time 20212.77ms, mfu 4.85%\n",
            "iter 653: loss 1.4624, time 20171.45ms, mfu 4.86%\n",
            "iter 654: loss 1.4074, time 20180.75ms, mfu 4.86%\n",
            "iter 655: loss 1.4932, time 20169.20ms, mfu 4.86%\n",
            "iter 656: loss 1.4454, time 20133.89ms, mfu 4.86%\n",
            "iter 657: loss 1.4845, time 20161.03ms, mfu 4.86%\n",
            "iter 658: loss 1.4433, time 20159.27ms, mfu 4.86%\n",
            "iter 659: loss 1.4857, time 20134.11ms, mfu 4.86%\n",
            "iter 660: loss 1.5193, time 20151.29ms, mfu 4.86%\n",
            "iter 661: loss 1.4961, time 20226.78ms, mfu 4.86%\n",
            "iter 662: loss 1.4883, time 20211.49ms, mfu 4.86%\n",
            "iter 663: loss 1.4655, time 20227.96ms, mfu 4.86%\n",
            "iter 664: loss 1.4481, time 20229.73ms, mfu 4.86%\n",
            "iter 665: loss 1.4941, time 20211.09ms, mfu 4.86%\n",
            "iter 666: loss 1.4766, time 20242.60ms, mfu 4.86%\n",
            "iter 667: loss 1.4467, time 20219.87ms, mfu 4.86%\n",
            "iter 668: loss 1.4815, time 20223.67ms, mfu 4.86%\n",
            "iter 669: loss 1.5454, time 20191.23ms, mfu 4.86%\n",
            "iter 670: loss 1.5006, time 20197.88ms, mfu 4.86%\n",
            "iter 671: loss 1.5574, time 20212.91ms, mfu 4.86%\n",
            "iter 672: loss 1.4291, time 20206.28ms, mfu 4.86%\n",
            "iter 673: loss 1.4803, time 20218.91ms, mfu 4.86%\n",
            "iter 674: loss 1.4424, time 20184.79ms, mfu 4.86%\n",
            "iter 675: loss 1.5321, time 20184.16ms, mfu 4.86%\n",
            "iter 676: loss 1.3851, time 20192.19ms, mfu 4.86%\n",
            "iter 677: loss 1.4443, time 20183.37ms, mfu 4.86%\n",
            "iter 678: loss 1.4502, time 20190.78ms, mfu 4.86%\n",
            "iter 679: loss 1.4882, time 20174.08ms, mfu 4.86%\n",
            "iter 680: loss 1.4228, time 20166.74ms, mfu 4.86%\n",
            "iter 681: loss 1.5029, time 20111.70ms, mfu 4.86%\n",
            "iter 682: loss 1.4618, time 20152.96ms, mfu 4.86%\n",
            "iter 683: loss 1.4336, time 20116.58ms, mfu 4.87%\n",
            "iter 684: loss 1.4189, time 20115.27ms, mfu 4.87%\n",
            "iter 685: loss 1.4502, time 20127.37ms, mfu 4.87%\n",
            "iter 686: loss 1.5473, time 20164.01ms, mfu 4.87%\n",
            "iter 687: loss 1.4383, time 20119.55ms, mfu 4.87%\n",
            "iter 688: loss 1.3869, time 20132.03ms, mfu 4.87%\n",
            "iter 689: loss 1.4685, time 20171.88ms, mfu 4.87%\n",
            "iter 690: loss 1.4854, time 20139.14ms, mfu 4.87%\n",
            "iter 691: loss 1.4141, time 20161.08ms, mfu 4.87%\n",
            "iter 692: loss 1.4589, time 20160.72ms, mfu 4.87%\n",
            "iter 693: loss 1.4862, time 20132.63ms, mfu 4.87%\n",
            "iter 694: loss 1.4479, time 20162.31ms, mfu 4.87%\n",
            "iter 695: loss 1.4860, time 20128.56ms, mfu 4.87%\n",
            "iter 696: loss 1.4555, time 20154.66ms, mfu 4.87%\n",
            "iter 697: loss 1.4276, time 20143.33ms, mfu 4.87%\n",
            "iter 698: loss 1.4097, time 20116.99ms, mfu 4.87%\n",
            "iter 699: loss 1.4488, time 20133.46ms, mfu 4.87%\n",
            "iter 700: loss 1.4292, time 20108.39ms, mfu 4.87%\n",
            "iter 701: loss 1.3789, time 20122.04ms, mfu 4.87%\n",
            "iter 702: loss 1.3817, time 20141.56ms, mfu 4.87%\n",
            "iter 703: loss 1.4226, time 20144.48ms, mfu 4.87%\n",
            "iter 704: loss 1.4216, time 20151.42ms, mfu 4.87%\n",
            "iter 705: loss 1.4527, time 20126.63ms, mfu 4.87%\n",
            "iter 706: loss 1.4631, time 20124.30ms, mfu 4.88%\n",
            "iter 707: loss 1.4615, time 20135.66ms, mfu 4.88%\n",
            "iter 708: loss 1.4267, time 20165.10ms, mfu 4.87%\n",
            "iter 709: loss 1.4087, time 20168.32ms, mfu 4.87%\n",
            "iter 710: loss 1.4242, time 20157.81ms, mfu 4.87%\n",
            "iter 711: loss 1.3881, time 20150.44ms, mfu 4.87%\n",
            "iter 712: loss 1.3885, time 20142.84ms, mfu 4.87%\n",
            "iter 713: loss 1.4161, time 20120.83ms, mfu 4.87%\n",
            "iter 714: loss 1.3888, time 20179.17ms, mfu 4.87%\n",
            "iter 715: loss 1.4320, time 20150.95ms, mfu 4.87%\n",
            "iter 716: loss 1.3702, time 20197.89ms, mfu 4.87%\n",
            "iter 717: loss 1.4347, time 20260.36ms, mfu 4.87%\n",
            "iter 718: loss 1.4260, time 20259.00ms, mfu 4.87%\n",
            "iter 719: loss 1.4899, time 20275.60ms, mfu 4.86%\n",
            "iter 720: loss 1.3994, time 20267.03ms, mfu 4.86%\n",
            "iter 721: loss 1.4484, time 20273.68ms, mfu 4.86%\n",
            "iter 722: loss 1.4104, time 20230.02ms, mfu 4.86%\n",
            "iter 723: loss 1.4124, time 20240.51ms, mfu 4.86%\n",
            "iter 724: loss 1.3558, time 20253.92ms, mfu 4.86%\n",
            "iter 725: loss 1.3656, time 20253.39ms, mfu 4.86%\n",
            "iter 726: loss 1.4462, time 20220.88ms, mfu 4.86%\n",
            "iter 727: loss 1.3797, time 20119.11ms, mfu 4.86%\n",
            "iter 728: loss 1.3888, time 20097.17ms, mfu 4.86%\n",
            "iter 729: loss 1.3810, time 20144.22ms, mfu 4.86%\n",
            "iter 730: loss 1.3036, time 20145.06ms, mfu 4.86%\n",
            "iter 731: loss 1.4311, time 20142.40ms, mfu 4.87%\n",
            "iter 732: loss 1.3955, time 20168.27ms, mfu 4.87%\n",
            "iter 733: loss 1.3601, time 20181.22ms, mfu 4.87%\n",
            "iter 734: loss 1.4238, time 20212.63ms, mfu 4.86%\n",
            "iter 735: loss 1.4201, time 20187.10ms, mfu 4.86%\n",
            "iter 736: loss 1.4222, time 20178.60ms, mfu 4.86%\n",
            "iter 737: loss 1.4277, time 20159.83ms, mfu 4.87%\n",
            "iter 738: loss 1.4116, time 20240.99ms, mfu 4.86%\n",
            "iter 739: loss 1.3895, time 20268.09ms, mfu 4.86%\n",
            "iter 740: loss 1.3066, time 20261.56ms, mfu 4.86%\n",
            "iter 741: loss 1.3213, time 20279.64ms, mfu 4.86%\n",
            "iter 742: loss 1.3652, time 20288.67ms, mfu 4.86%\n",
            "iter 743: loss 1.3889, time 20234.58ms, mfu 4.86%\n",
            "iter 744: loss 1.4373, time 20226.10ms, mfu 4.86%\n",
            "iter 745: loss 1.3500, time 20248.85ms, mfu 4.86%\n",
            "iter 746: loss 1.3665, time 20214.93ms, mfu 4.86%\n",
            "iter 747: loss 1.3764, time 20227.31ms, mfu 4.86%\n",
            "iter 748: loss 1.2986, time 20217.80ms, mfu 4.86%\n",
            "iter 749: loss 1.3271, time 20193.98ms, mfu 4.86%\n",
            "iter 750: loss 1.3563, time 20236.21ms, mfu 4.86%\n",
            "iter 751: loss 1.4111, time 20163.92ms, mfu 4.86%\n",
            "iter 752: loss 1.4468, time 20175.63ms, mfu 4.86%\n",
            "iter 753: loss 1.4205, time 20158.65ms, mfu 4.86%\n",
            "iter 754: loss 1.3126, time 20177.34ms, mfu 4.86%\n",
            "iter 755: loss 1.3887, time 20167.02ms, mfu 4.86%\n",
            "iter 756: loss 1.3954, time 20169.43ms, mfu 4.86%\n",
            "iter 757: loss 1.3887, time 20182.40ms, mfu 4.86%\n",
            "iter 758: loss 1.3976, time 20171.67ms, mfu 4.86%\n",
            "iter 759: loss 1.4133, time 20152.30ms, mfu 4.86%\n",
            "iter 760: loss 1.4028, time 20170.32ms, mfu 4.86%\n",
            "iter 761: loss 1.3562, time 20162.06ms, mfu 4.86%\n",
            "iter 762: loss 1.3491, time 20159.26ms, mfu 4.86%\n",
            "iter 763: loss 1.3156, time 20197.62ms, mfu 4.86%\n",
            "iter 764: loss 1.2992, time 20192.22ms, mfu 4.86%\n",
            "iter 765: loss 1.3898, time 20187.59ms, mfu 4.86%\n",
            "iter 766: loss 1.3228, time 20201.49ms, mfu 4.86%\n",
            "iter 767: loss 1.3235, time 20201.11ms, mfu 4.86%\n",
            "iter 768: loss 1.3703, time 20169.24ms, mfu 4.86%\n",
            "iter 769: loss 1.4173, time 20203.96ms, mfu 4.86%\n",
            "iter 770: loss 1.3522, time 20171.21ms, mfu 4.86%\n",
            "iter 771: loss 1.3920, time 20173.06ms, mfu 4.86%\n",
            "iter 772: loss 1.3877, time 20144.20ms, mfu 4.87%\n",
            "iter 773: loss 1.3464, time 20183.54ms, mfu 4.87%\n",
            "iter 774: loss 1.3287, time 20192.09ms, mfu 4.86%\n",
            "iter 775: loss 1.3338, time 20172.10ms, mfu 4.87%\n",
            "iter 776: loss 1.3254, time 20148.16ms, mfu 4.87%\n",
            "iter 777: loss 1.3726, time 20169.56ms, mfu 4.87%\n",
            "iter 778: loss 1.2860, time 20165.89ms, mfu 4.87%\n",
            "iter 779: loss 1.3370, time 20158.38ms, mfu 4.87%\n",
            "iter 780: loss 1.3739, time 20194.85ms, mfu 4.87%\n",
            "iter 781: loss 1.3778, time 20188.89ms, mfu 4.87%\n",
            "iter 782: loss 1.3300, time 20174.89ms, mfu 4.87%\n",
            "iter 783: loss 1.3627, time 20167.03ms, mfu 4.87%\n",
            "iter 784: loss 1.3485, time 20170.04ms, mfu 4.87%\n",
            "iter 785: loss 1.3673, time 20185.89ms, mfu 4.87%\n",
            "iter 786: loss 1.2985, time 20171.98ms, mfu 4.87%\n",
            "iter 787: loss 1.3441, time 20193.76ms, mfu 4.87%\n",
            "iter 788: loss 1.3712, time 20196.49ms, mfu 4.87%\n",
            "iter 789: loss 1.3142, time 20163.49ms, mfu 4.87%\n",
            "iter 790: loss 1.2915, time 20181.10ms, mfu 4.87%\n",
            "iter 791: loss 1.3288, time 20207.80ms, mfu 4.87%\n",
            "iter 792: loss 1.3172, time 20173.90ms, mfu 4.87%\n",
            "iter 793: loss 1.3179, time 20207.81ms, mfu 4.86%\n",
            "iter 794: loss 1.3165, time 20188.81ms, mfu 4.86%\n",
            "iter 795: loss 1.3654, time 20210.29ms, mfu 4.86%\n",
            "iter 796: loss 1.3419, time 20182.82ms, mfu 4.86%\n",
            "iter 797: loss 1.3275, time 20143.72ms, mfu 4.86%\n",
            "iter 798: loss 1.3589, time 20159.05ms, mfu 4.87%\n",
            "iter 799: loss 1.2899, time 20180.09ms, mfu 4.87%\n",
            "iter 800: loss 1.3433, time 20194.21ms, mfu 4.87%\n",
            "iter 801: loss 1.2958, time 20177.75ms, mfu 4.87%\n",
            "iter 802: loss 1.2858, time 20158.30ms, mfu 4.87%\n",
            "iter 803: loss 1.4433, time 20170.80ms, mfu 4.87%\n",
            "iter 804: loss 1.3390, time 20196.54ms, mfu 4.87%\n",
            "iter 805: loss 1.3951, time 20161.17ms, mfu 4.87%\n",
            "iter 806: loss 1.3413, time 20186.67ms, mfu 4.87%\n",
            "iter 807: loss 1.3079, time 20186.86ms, mfu 4.87%\n",
            "iter 808: loss 1.3550, time 20211.92ms, mfu 4.86%\n",
            "iter 809: loss 1.3224, time 20184.57ms, mfu 4.86%\n",
            "iter 810: loss 1.3425, time 20197.55ms, mfu 4.86%\n",
            "iter 811: loss 1.2993, time 20191.57ms, mfu 4.86%\n",
            "iter 812: loss 1.2958, time 20257.05ms, mfu 4.86%\n",
            "iter 813: loss 1.3230, time 20255.42ms, mfu 4.86%\n",
            "iter 814: loss 1.4031, time 20253.34ms, mfu 4.86%\n",
            "iter 815: loss 1.3489, time 20268.86ms, mfu 4.86%\n",
            "iter 816: loss 1.3573, time 20307.99ms, mfu 4.86%\n",
            "iter 817: loss 1.2220, time 20289.70ms, mfu 4.85%\n",
            "iter 818: loss 1.3671, time 20286.40ms, mfu 4.85%\n",
            "iter 819: loss 1.3406, time 20259.93ms, mfu 4.85%\n",
            "iter 820: loss 1.3601, time 20249.64ms, mfu 4.85%\n",
            "iter 821: loss 1.3077, time 20248.67ms, mfu 4.85%\n",
            "iter 822: loss 1.3139, time 20243.09ms, mfu 4.85%\n",
            "iter 823: loss 1.2807, time 20245.96ms, mfu 4.85%\n",
            "iter 824: loss 1.2830, time 20228.23ms, mfu 4.85%\n",
            "iter 825: loss 1.3061, time 20284.66ms, mfu 4.85%\n",
            "iter 826: loss 1.2878, time 20194.01ms, mfu 4.85%\n",
            "iter 827: loss 1.2971, time 20235.07ms, mfu 4.85%\n",
            "iter 828: loss 1.2774, time 20209.63ms, mfu 4.85%\n",
            "iter 829: loss 1.3072, time 20214.25ms, mfu 4.85%\n",
            "iter 830: loss 1.3130, time 20222.33ms, mfu 4.85%\n",
            "iter 831: loss 1.2464, time 20177.37ms, mfu 4.85%\n",
            "iter 832: loss 1.2990, time 20175.97ms, mfu 4.86%\n",
            "iter 833: loss 1.2902, time 20171.47ms, mfu 4.86%\n",
            "iter 834: loss 1.2615, time 20166.01ms, mfu 4.86%\n",
            "iter 835: loss 1.3033, time 20167.78ms, mfu 4.86%\n",
            "iter 836: loss 1.2428, time 20173.30ms, mfu 4.86%\n",
            "iter 837: loss 1.2993, time 20188.07ms, mfu 4.86%\n",
            "iter 838: loss 1.3490, time 20178.79ms, mfu 4.86%\n",
            "iter 839: loss 1.2768, time 20176.79ms, mfu 4.86%\n",
            "iter 840: loss 1.3011, time 20163.95ms, mfu 4.86%\n",
            "iter 841: loss 1.2435, time 20162.05ms, mfu 4.86%\n",
            "iter 842: loss 1.2973, time 20169.15ms, mfu 4.86%\n",
            "iter 843: loss 1.2938, time 20154.42ms, mfu 4.86%\n",
            "iter 844: loss 1.2487, time 20199.53ms, mfu 4.86%\n",
            "iter 845: loss 1.2739, time 20177.30ms, mfu 4.86%\n",
            "iter 846: loss 1.3431, time 20154.33ms, mfu 4.86%\n",
            "iter 847: loss 1.2794, time 20169.98ms, mfu 4.87%\n",
            "iter 848: loss 1.2622, time 20152.21ms, mfu 4.87%\n",
            "iter 849: loss 1.2947, time 20177.18ms, mfu 4.87%\n",
            "iter 850: loss 1.3249, time 20144.97ms, mfu 4.87%\n",
            "iter 851: loss 1.3323, time 20135.33ms, mfu 4.87%\n",
            "iter 852: loss 1.3192, time 20161.56ms, mfu 4.87%\n",
            "iter 853: loss 1.2840, time 20153.62ms, mfu 4.87%\n",
            "iter 854: loss 1.2057, time 20186.80ms, mfu 4.87%\n",
            "iter 855: loss 1.2590, time 20127.46ms, mfu 4.87%\n",
            "iter 856: loss 1.3007, time 20135.21ms, mfu 4.87%\n",
            "iter 857: loss 1.2728, time 20167.42ms, mfu 4.87%\n",
            "iter 858: loss 1.2641, time 20165.93ms, mfu 4.87%\n",
            "iter 859: loss 1.2851, time 20197.43ms, mfu 4.87%\n",
            "iter 860: loss 1.3121, time 20161.64ms, mfu 4.87%\n",
            "iter 861: loss 1.2969, time 20148.45ms, mfu 4.87%\n",
            "iter 862: loss 1.2989, time 20200.55ms, mfu 4.87%\n",
            "iter 863: loss 1.2339, time 20180.70ms, mfu 4.87%\n",
            "iter 864: loss 1.2508, time 20188.45ms, mfu 4.87%\n",
            "iter 865: loss 1.2416, time 20174.36ms, mfu 4.87%\n",
            "iter 866: loss 1.4128, time 20176.66ms, mfu 4.87%\n",
            "iter 867: loss 1.2756, time 20178.47ms, mfu 4.87%\n",
            "iter 868: loss 1.2678, time 20231.75ms, mfu 4.87%\n",
            "iter 869: loss 1.3110, time 20196.85ms, mfu 4.87%\n",
            "iter 870: loss 1.3546, time 20217.27ms, mfu 4.86%\n",
            "iter 871: loss 1.2662, time 20276.20ms, mfu 4.86%\n",
            "iter 872: loss 1.2868, time 20278.10ms, mfu 4.86%\n",
            "iter 873: loss 1.2873, time 20286.53ms, mfu 4.86%\n",
            "iter 874: loss 1.2638, time 20258.88ms, mfu 4.86%\n",
            "iter 875: loss 1.2902, time 20216.70ms, mfu 4.86%\n",
            "iter 876: loss 1.2824, time 20205.00ms, mfu 4.86%\n",
            "iter 877: loss 1.2587, time 20190.16ms, mfu 4.86%\n",
            "iter 878: loss 1.3134, time 20177.58ms, mfu 4.86%\n",
            "iter 879: loss 1.3280, time 20146.37ms, mfu 4.86%\n",
            "iter 880: loss 1.2972, time 20159.27ms, mfu 4.86%\n",
            "iter 881: loss 1.2576, time 20164.47ms, mfu 4.86%\n",
            "iter 882: loss 1.2340, time 20164.32ms, mfu 4.86%\n",
            "iter 883: loss 1.2306, time 20105.19ms, mfu 4.86%\n",
            "iter 884: loss 1.2997, time 20160.47ms, mfu 4.87%\n",
            "iter 885: loss 1.2000, time 20135.05ms, mfu 4.87%\n",
            "iter 886: loss 1.2590, time 20111.59ms, mfu 4.87%\n",
            "iter 887: loss 1.3240, time 20108.29ms, mfu 4.87%\n",
            "iter 888: loss 1.2186, time 20135.91ms, mfu 4.87%\n",
            "iter 889: loss 1.2323, time 20109.62ms, mfu 4.87%\n",
            "iter 890: loss 1.2752, time 20110.02ms, mfu 4.87%\n",
            "iter 891: loss 1.1862, time 20144.65ms, mfu 4.87%\n",
            "iter 892: loss 1.2405, time 20111.07ms, mfu 4.87%\n",
            "iter 893: loss 1.2192, time 20153.47ms, mfu 4.87%\n",
            "iter 894: loss 1.2614, time 20129.15ms, mfu 4.87%\n",
            "iter 895: loss 1.2800, time 20150.64ms, mfu 4.87%\n",
            "iter 896: loss 1.2524, time 20166.27ms, mfu 4.87%\n",
            "iter 897: loss 1.1753, time 20161.56ms, mfu 4.87%\n",
            "iter 898: loss 1.3195, time 20197.94ms, mfu 4.87%\n",
            "iter 899: loss 1.2333, time 20234.18ms, mfu 4.87%\n",
            "iter 900: loss 1.2259, time 20212.54ms, mfu 4.87%\n",
            "iter 901: loss 1.2348, time 20205.34ms, mfu 4.87%\n",
            "iter 902: loss 1.2109, time 20214.41ms, mfu 4.87%\n",
            "iter 903: loss 1.2289, time 20242.08ms, mfu 4.86%\n",
            "iter 904: loss 1.2580, time 20212.44ms, mfu 4.86%\n",
            "iter 905: loss 1.2297, time 20225.23ms, mfu 4.86%\n",
            "iter 906: loss 1.2399, time 20192.17ms, mfu 4.86%\n",
            "iter 907: loss 1.2246, time 20219.17ms, mfu 4.86%\n",
            "iter 908: loss 1.2133, time 20200.55ms, mfu 4.86%\n",
            "iter 909: loss 1.1945, time 20197.11ms, mfu 4.86%\n",
            "iter 910: loss 1.2405, time 20217.48ms, mfu 4.86%\n",
            "iter 911: loss 1.2154, time 20201.82ms, mfu 4.86%\n",
            "iter 912: loss 1.2107, time 20203.55ms, mfu 4.86%\n",
            "iter 913: loss 1.2652, time 20157.58ms, mfu 4.86%\n",
            "iter 914: loss 1.2115, time 20166.03ms, mfu 4.86%\n",
            "iter 915: loss 1.2493, time 20157.52ms, mfu 4.86%\n",
            "iter 916: loss 1.2053, time 20200.98ms, mfu 4.86%\n",
            "iter 917: loss 1.2039, time 20196.67ms, mfu 4.86%\n",
            "iter 918: loss 1.1634, time 20228.18ms, mfu 4.86%\n",
            "iter 919: loss 1.2124, time 20246.37ms, mfu 4.86%\n",
            "iter 920: loss 1.1956, time 20279.14ms, mfu 4.86%\n",
            "iter 921: loss 1.1958, time 20253.50ms, mfu 4.86%\n",
            "iter 922: loss 1.1717, time 20266.52ms, mfu 4.86%\n",
            "iter 923: loss 1.2148, time 20237.64ms, mfu 4.86%\n",
            "iter 924: loss 1.2314, time 20213.07ms, mfu 4.86%\n",
            "iter 925: loss 1.1941, time 20204.99ms, mfu 4.86%\n",
            "iter 926: loss 1.2575, time 20189.58ms, mfu 4.86%\n",
            "iter 927: loss 1.2633, time 20143.95ms, mfu 4.86%\n",
            "iter 928: loss 1.2221, time 20185.36ms, mfu 4.86%\n",
            "iter 929: loss 1.2107, time 20139.93ms, mfu 4.86%\n",
            "iter 930: loss 1.2135, time 20165.21ms, mfu 4.86%\n",
            "iter 931: loss 1.1641, time 20144.38ms, mfu 4.86%\n",
            "iter 932: loss 1.2463, time 20154.13ms, mfu 4.86%\n",
            "iter 933: loss 1.2202, time 20185.59ms, mfu 4.86%\n",
            "iter 934: loss 1.2293, time 20225.27ms, mfu 4.86%\n",
            "iter 935: loss 1.2000, time 20230.49ms, mfu 4.86%\n",
            "iter 936: loss 1.2123, time 20215.01ms, mfu 4.86%\n",
            "iter 937: loss 1.1785, time 20233.66ms, mfu 4.86%\n",
            "iter 938: loss 1.1641, time 20254.86ms, mfu 4.86%\n",
            "iter 939: loss 1.1921, time 20259.93ms, mfu 4.86%\n",
            "iter 940: loss 1.2002, time 20253.35ms, mfu 4.86%\n",
            "iter 941: loss 1.1996, time 20163.65ms, mfu 4.86%\n",
            "iter 942: loss 1.2051, time 20090.29ms, mfu 4.86%\n",
            "iter 943: loss 1.2361, time 20144.15ms, mfu 4.86%\n",
            "iter 944: loss 1.1964, time 20209.69ms, mfu 4.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PODSEvWh4joG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}